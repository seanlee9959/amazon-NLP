{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanlee9959/amazon-NLP/blob/main/amzn_webscraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Webscraper."
      ],
      "metadata": {
        "id": "prucqXdNdChk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfxcDy0wdSMH"
      },
      "outputs": [],
      "source": [
        "#%% Code\n",
        "\n",
        "\n",
        "# Import packages\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Create chrome instance and navigate to website\n",
        "options = ChromeOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "driver = webdriver.Chrome(executable_path='C:/Users/cjconst/Documents/ECON 370/chromedriver.exe', options=options)\n",
        "search = \"microwave\"\n",
        "url_template = \"https://www.amazon.com/s?k={}&page={}\"\n",
        "\n",
        "# Defining an empty list of urls and first page of search\n",
        "urls = []\n",
        "page = 1\n",
        "\n",
        "# Navigating to page 1 and define the maximum number of pages\n",
        "url = url_template.format(search, page)\n",
        "driver.get(url)\n",
        "\n",
        "max_page_element = driver.find_elements_by_class_name(\"s-pagination-item.s-pagination-disabled\")\n",
        "max_page = []\n",
        "for element in max_page_element:\n",
        "    element = element.text.strip()\n",
        "    max_page.append(element)\n",
        "max_page = max_page[1]\n",
        "max_page = int(max_page)\n",
        "\n",
        "# Invalid strings to exclude from URLs\n",
        "invalid_strings = [\"offer-listing\", \"bestsellers\", \"ospublishing\", \"#customerReviews\", (\"s?k=\" + search)]\n",
        "\n",
        "# Loop over pages until we reach the maximum number of pages in the search\n",
        "while page <= max_page:\n",
        "    url = url_template.format(search, page)\n",
        "    driver.get(url)\n",
        "    \n",
        "    # Find all the links on the page\n",
        "    links = driver.find_elements_by_class_name('a-link-normal.s-underline-text.s-underline-link-text')\n",
        "\n",
        "    # Iterate over the links and add them to the list\n",
        "    for link in links:\n",
        "        href = link.get_attribute(\"href\")\n",
        "        url_valid = True\n",
        "        for invalid_string in invalid_strings:\n",
        "            if invalid_string in href or search not in href:\n",
        "                url_valid = False\n",
        "                break\n",
        "        if url_valid and href not in urls:\n",
        "            urls.append(href)\n",
        "            \n",
        "    # Increase index of page\n",
        "    page += 1\n",
        "\n",
        "# create empty list for writing observations to\n",
        "data = []\n",
        "\n",
        "# Extracting information from each url\n",
        "for url in urls:\n",
        "    # Connecting to URL\n",
        "    driver.get(url)\n",
        "    # Finding the product ASIN by finding a match\n",
        "    try:\n",
        "        product_details_elements = driver.find_elements_by_class_name('a-size-base.prodDetAttrValue')\n",
        "        product_details = []\n",
        "        for element in product_details_elements:\n",
        "            element = element.text.strip()\n",
        "            if (len(element) == 10) & (element[0] == \"B\"):\n",
        "                product_details.append(element)\n",
        "        asin = product_details[0]\n",
        "    except:\n",
        "        asin = \"NaN\"\n",
        "    \n",
        "    # Finding the product name element and extracting its text\n",
        "    try:\n",
        "        name_element = driver.find_element_by_id('productTitle')\n",
        "        name = name_element.text.strip()\n",
        "    except:\n",
        "        name = \"NaN\"\n",
        "        \n",
        "    # Finding the brand\n",
        "    try:\n",
        "        brand_element = driver.find_element_by_class_name('a-spacing-small.po-brand')\n",
        "        brand = brand_element.text.strip()\n",
        "    except:\n",
        "        brand = \"NaN\"\n",
        "    \n",
        "    # Finding the list price\n",
        "    try:\n",
        "        list_price_element = driver.find_element_by_class_name('basisPrice')\n",
        "        list_price = list_price_element.text.strip()\n",
        "        list_price = list_price.split('$')[1]\n",
        "    except:\n",
        "        list_price = \"NaN\"\n",
        "    \n",
        "    # Finding the buy price\n",
        "    try:\n",
        "        buy_price_element = driver.find_element_by_class_name('priceToPay')\n",
        "        buy_price = buy_price_element.text.strip()\n",
        "        buy_price = buy_price.split()[0] + '.' + buy_price.split()[1]\n",
        "        buy_price = buy_price[1:]\n",
        "    try:\n",
        "        buy_price_element = driver.find_element_by_class_name('a-color-price.a-text-bold')\n",
        "        buy_price = buy_price_element.text.strip()\n",
        "    except:\n",
        "        buy_price = \"NaN\"\n",
        "    \n",
        "    \n",
        "    # Finding the Prime exclusive price message and extract its text\n",
        "    try:\n",
        "        prime_price_element = driver.find_element_by_id('primeExclusivePricingMessage')\n",
        "        prime_price = prime_price_element.text.strip()\n",
        "        prime_price = prime_price.split('$')[1]\n",
        "    except:\n",
        "        prime_price = \"NaN\"\n",
        "    \n",
        "    # Finding the Average Rating\n",
        "    try:\n",
        "        rating_elements = driver.find_element_by_css_selector('[data-hook=\"rating-out-of-text\"]')\n",
        "        rating = rating_elements.text.strip()[:3]\n",
        "    except:\n",
        "        rating = \"NaN\"\n",
        "    \n",
        "    # Finding the count of ratings\n",
        "    try:\n",
        "        count_element = driver.find_element_by_id('acrCustomerReviewText')\n",
        "        count = count_element.text.strip()\n",
        "        count = count.split()[0]\n",
        "    except:\n",
        "        count = \"NaN\"\n",
        "    \n",
        "    # Finding the product description and extracting text elements as a list\n",
        "    try:\n",
        "        product_description_elements = driver.find_elements_by_id('feature-bullets')\n",
        "        product_description = []\n",
        "        for element in product_description_elements:\n",
        "            element = element.text.strip()\n",
        "            product_description.append(element)\n",
        "        # Making product description a single string\n",
        "        product_description = ''.join(product_description)\n",
        "        # Removing special unicode characters that cannot be coded as ascii\n",
        "        product_description = product_description.encode('ascii', 'ignore').decode('ascii')\n",
        "    except:\n",
        "        product_description = \"NaN\"\n",
        "    \n",
        "    # Finding the \"ships from\" label\n",
        "    try:\n",
        "        shipper_seller_elements = driver.find_elements_by_class_name('tabular-buybox-text.a-spacing-none')\n",
        "        shipper_seller = []\n",
        "        for element in shipper_seller_elements:\n",
        "            element = element.text.strip()\n",
        "            shipper_seller.append(element)\n",
        "        shipper = shipper_seller[0]\n",
        "        seller = shipper_seller[1]\n",
        "    except:\n",
        "        shipper = \"NaN\"\n",
        "        seller = \"NaN\"\n",
        "        \n",
        "    # Finding the product category\n",
        "    try:\n",
        "        category_element = driver.find_element_by_id('wayfinding-breadcrumbs_feature_div')\n",
        "        category = category_element.text.strip()\n",
        "        category = category.split(\"â€º\")\n",
        "        category = category.pop()\n",
        "    except:\n",
        "        category = \"NaN\"\n",
        "    \n",
        "    data.append([asin, name, brand, list_price, buy_price, prime_price, rating, count, product_description, shipper, seller, category, url])\n",
        "  \n",
        "# Close the Firefox window\n",
        "driver.quit()\n",
        "\n",
        "# Create dataframe and export as csv\n",
        "df = pd.DataFrame(data)   \n",
        "df.columns = ['asin', 'name', 'brand', 'list_price', 'buy_price', 'prime_price', 'rating', 'count', 'product_description', 'shipper', 'seller', 'category', 'url']\n",
        "df.to_csv((search + '.csv'), encoding='utf-8', index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample of EDA and data cleaning. Y'all's might have to look different to accomodate for different data but look through this for anything you might need. "
      ],
      "metadata": {
        "id": "8R8B_Mf4dVc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% EDA and Cleaning\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reading in data\n",
        "os.chdir(\"C:/Users/cjconst/Documents/ECON 370\")\n",
        "microwaves = pd.read_csv(\"microwave.csv\")\n",
        "\n",
        "# Printing head of data\n",
        "head = microwaves.head()\n",
        "datatypes = microwaves.dtypes\n",
        "print(datatypes)\n",
        "\n",
        "# Converting all columns to string variables\n",
        "microwaves = microwaves.astype(str).replace('nan', None)\n",
        "\n",
        "# Removing commas from strings\n",
        "microwaves['list_price'] = microwaves['list_price'].str.replace(',','')\n",
        "microwaves['buy_price'] = microwaves['buy_price'].str.replace(',','')\n",
        "microwaves['prime_price'] = microwaves['prime_price'].str.replace(',','')\n",
        "microwaves['count'] = microwaves['count'].str.replace(',','')\n",
        "\n",
        "# FIX \"5 o\" PROBLEM\n",
        "microwaves['rating'] = microwaves['rating'].str.replace(' o','')\n",
        "\n",
        "# Fix space on categories\n",
        "microwaves['category'] = microwaves['category'].replace(r'\\n','', regex=True) \n",
        "\n",
        "# Removing 'brand' from brand\n",
        "microwaves['brand'] = microwaves['brand'].str.replace('Brand ', '')\n",
        "\n",
        "# Creating a dictionary for column conversions\n",
        "convert_dict = {'list_price': float,\n",
        "                'buy_price': float,\n",
        "                'prime_price': float,\n",
        "                'rating': float,\n",
        "                'count': float\n",
        "                }\n",
        "microwaves = microwaves.astype(convert_dict)\n",
        "print(microwaves.dtypes)\n",
        "\n",
        "# Summary info\n",
        "microwaves.info()\n",
        "microwaves.describe()\n",
        "\n",
        "# Find unique values of brand, shipper, seller, and category\n",
        "microwaves['brand'].unique()\n",
        "microwaves[\"shipper\"].unique()\n",
        "microwaves[\"seller\"].unique()\n",
        "microwaves[\"category\"].unique()\n",
        "\n",
        "# Plot the unique values\n",
        "microwaves['brand'].value_counts()[:20].plot(kind='bar')\n",
        "microwaves['shipper'].value_counts()[:10].plot(kind='bar')\n",
        "microwaves['seller'].value_counts()[:10].plot(kind='bar')\n",
        "microwaves['category'].value_counts()[:10].plot(kind='bar')\n",
        "\n",
        "# Only keeping actual microwaves\n",
        "filter_ = microwaves['category'].isin([\"Countertop Microwave Ovens\", 'Over-the-Range Microwave Ovens', \"Compact Microwave Ovens\"])\n",
        "microwaves = microwaves[filter_]\n",
        "microwaves['category'] = microwaves['category'].astype(\"category\")\n",
        "\n",
        "# Reviewing nulls\n",
        "microwaves.isnull().sum()\n",
        "\n",
        "# Fixing the null ASINs\n",
        "microwaves = microwaves.reset_index(drop=True)\n",
        "for idx, x in enumerate(microwaves[\"asin\"]):\n",
        "    if x == None:      \n",
        "        s = re.findall('dp/B.*/', microwaves[\"url\"][idx])\n",
        "        microwaves[\"asin\"][idx] = s[0][3:13]\n",
        "        \n",
        "# Defining NA counts as zero\n",
        "microwaves['count'].replace(np.nan,'0',inplace = True)\n",
        "microwaves['count'] = microwaves['count'].astype(int)\n",
        "\n",
        "# Defining NA ratings as mean of ratings\n",
        "microwaves['rating'].replace(np.nan, microwaves['rating'].mean(),inplace = True)\n",
        "microwaves['rating'] = microwaves['rating'].astype(float)\n",
        "        \n",
        "# Defining discount columns\n",
        "microwaves[\"discount\"] = (microwaves[\"list_price\"] - microwaves[\"buy_price\"])/microwaves[\"list_price\"]\n",
        "microwaves[\"prime_discount\"] = (microwaves[\"buy_price\"] - microwaves[\"prime_price\"])/microwaves[\"buy_price\"]\n",
        "\n",
        "# Defining NA discounts as zero\n",
        "microwaves['discount'].replace(np.nan, '0',inplace = True)\n",
        "microwaves['discount'] = microwaves['discount'].astype(float)\n",
        "\n",
        "# Defining NA brands as None\n",
        "microwaves['brand'].replace(np.nan,'None',inplace = True)\n",
        "\n",
        "# Defining Shipped by Amazon and Sold by Amazon Columns\n",
        "def amazon(val):\n",
        "    if val in ['Amazon', 'Amazon.com', 'Amazon Global Store UK']:\n",
        "        return 1 \n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "microwaves[\"shipper\"] = microwaves[\"shipper\"].apply(amazon)\n",
        "microwaves[\"seller\"] = microwaves[\"seller\"].apply(amazon)\n",
        "\n",
        "# Dropping list and prime prices\n",
        "microwaves = microwaves.drop([\"list_price\",\"prime_price\"],axis=1)\n",
        "microwaves = microwaves.reset_index()\n",
        "\n",
        "# Renaming columns\n",
        "microwaves.rename(columns={'buy_price':'price','index':'search_order'}, inplace = True)\n",
        "microwaves[\"search_order\"] = microwaves[\"search_order\"].astype(int)\n",
        "\n",
        "# Group means\n",
        "microwaves.groupby(['brand'])['price'].mean()\n",
        "microwaves.groupby(['shipper'])['price'].mean()\n",
        "microwaves.groupby(['seller'])['price'].mean()\n",
        "microwaves.groupby(['category'])['price'].mean()\n",
        "\n",
        "# Scatter plot visualizations\n",
        "plt.scatter(microwaves[\"search_order\"], microwaves[\"price\"], alpha=0.5)\n",
        "plt.scatter(microwaves[\"rating\"], microwaves[\"price\"], alpha=0.5)\n",
        "plt.scatter(np.log(microwaves[\"count\"]), microwaves[\"price\"], alpha=0.5)\n",
        "plt.scatter(microwaves[\"discount\"], microwaves[\"price\"], alpha=0.5)\n",
        "\n",
        "# Create a log price variable\n",
        "microwaves[\"logprice\"] = np.log(microwaves[\"price\"])\n",
        "\n",
        "# Writing csv of clean product\n",
        "microwaves.to_csv('microwaves_clean.csv',index=False) "
      ],
      "metadata": {
        "id": "93qgF4QvficG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block is all of my modeling stuff. This should be pretty much 1:1 for all of us."
      ],
      "metadata": {
        "id": "lba-V4qCdscw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Creating dataframe with just prices and titles\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.chdir(\"C:/Users/cjconst/Documents/ECON 370\")\n",
        "microwaves = pd.read_csv(\"microwaves_clean.csv\")\n",
        "\n",
        "microwaves.head()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# with smooth td-idf\n",
        "\n",
        "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
        "                        smooth_idf=True,  \n",
        "                        ngram_range=(2,2),\n",
        "                        stop_words='english',\n",
        "                        min_df=3)\n",
        " \n",
        " \n",
        "tf_idf = tf_idf_vec_smooth.fit_transform(microwaves[\"name\"])\n",
        " \n",
        "tf_idf = pd.DataFrame(tf_idf.toarray(),columns=tf_idf_vec_smooth.get_feature_names_out())\n",
        "\n",
        "tf_idf_scores = pd.DataFrame(tf_idf.mean())\n",
        "\n",
        "# remerging data\n",
        "microwaves = microwaves[[\"logprice\",\"search_order\",\"brand\",\"rating\",\"count\",\"shipper\",\"seller\",\"category\",\"discount\"]]\n",
        "microwaves = microwaves.merge(tf_idf, left_index=True, right_index=True)\n",
        "h=microwaves.head()\n",
        "\n",
        "# lasso regression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import Lasso, LinearRegression\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# Defining dummies for brand and category\n",
        "microwaves = microwaves.merge(pd.get_dummies(microwaves[[\"brand\",\"category\"]]), left_index=True, right_index=True)\n",
        "microwaves = microwaves.drop(columns=[\"brand\",\"category\"])\n",
        "\n",
        "# Drop NAs and scale variables\n",
        "microwaves = microwaves.dropna()\n",
        "scaler = StandardScaler()\n",
        "microwaves_scaled = scaler.fit_transform(microwaves)\n",
        "microwaves_scaled = pd.DataFrame(data=microwaves_scaled,columns=microwaves.columns)\n",
        "\n",
        "# Defining X and Y\n",
        "Y = microwaves_scaled[\"logprice\"].values\n",
        "X = microwaves_scaled.drop(columns=\"logprice\").values\n",
        "\n",
        "# Train and test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=123)\n",
        "\n",
        "# Describe data\n",
        "Y_test.mean()\n",
        "Y_test.std()\n",
        "Y_test.min()\n",
        "Y_test.max()\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = microwaves_scaled.corr()\n",
        "sns.heatmap(corr_matrix)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#%% LASSO Function Definitions\n",
        "# - From Mate Pocs: https://towardsdatascience.com/hyperparameter-tuning-in-lasso-and-ridge-regressions-70a4b158ae6d\n",
        "\n",
        "def regmodel_param_plot(\n",
        "    validation_score, train_score, alphas_to_try, chosen_alpha,\n",
        "    scoring, model_name, test_score = None, filename = None):\n",
        "    \n",
        "    plt.figure(figsize = (8,8))\n",
        "    sns.lineplot(y = validation_score, x = alphas_to_try, \n",
        "                 label = 'validation_data')\n",
        "    sns.lineplot(y = train_score, x = alphas_to_try, \n",
        "                 label = 'training_data')\n",
        "    plt.axvline(x=chosen_alpha, linestyle='--')\n",
        "    if test_score is not None:\n",
        "        sns.lineplot(y = test_score, x = alphas_to_try, \n",
        "                     label = 'test_data')\n",
        "    plt.xlabel('alpha_parameter')\n",
        "    plt.ylabel(scoring)\n",
        "    plt.title(model_name + ' Regularisation')\n",
        "    plt.legend()\n",
        "    if filename is not None:\n",
        "        plt.savefig(str(filename) + \".png\")\n",
        "    plt.show()\n",
        "    \n",
        "def regmodel_param_test(\n",
        "    alphas_to_try, X, y, cv, scoring = 'r2', \n",
        "    model_name = 'LASSO', X_test = None, y_test = None, \n",
        "    draw_plot = False, filename = None):\n",
        "    \n",
        "    validation_scores = []\n",
        "    train_scores = []\n",
        "    results_list = []\n",
        "    if X_test is not None:\n",
        "        test_scores = []\n",
        "        scorer = get_scorer(scoring)\n",
        "    else:\n",
        "        test_scores = None\n",
        "\n",
        "    for curr_alpha in alphas_to_try:\n",
        "        \n",
        "        if model_name == 'LASSO':\n",
        "            regmodel = Lasso(alpha = curr_alpha)\n",
        "        elif model_name == 'Ridge':\n",
        "            regmodel = Ridge(alpha = curr_alpha)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        results = cross_validate(\n",
        "            regmodel, X, y, scoring=scoring, cv=cv, \n",
        "            return_train_score = True)\n",
        "\n",
        "        validation_scores.append(np.mean(results['test_score']))\n",
        "        train_scores.append(np.mean(results['train_score']))\n",
        "        results_list.append(results)\n",
        "\n",
        "        if X_test is not None:\n",
        "            regmodel.fit(X,y)\n",
        "            y_pred = regmodel.predict(X_test)\n",
        "            test_scores.append(scorer(regmodel, X_test, y_test))\n",
        "    \n",
        "    chosen_alpha_id = np.argmax(validation_scores)\n",
        "    chosen_alpha = alphas_to_try[chosen_alpha_id]\n",
        "    max_validation_score = np.max(validation_scores)\n",
        "    if X_test is not None:\n",
        "        test_score_at_chosen_alpha = test_scores[chosen_alpha_id]\n",
        "    else:\n",
        "        test_score_at_chosen_alpha = None\n",
        "        \n",
        "    if draw_plot:\n",
        "        regmodel_param_plot(\n",
        "            validation_scores, train_scores, alphas_to_try, chosen_alpha, \n",
        "            scoring, model_name, test_scores, filename)\n",
        "    \n",
        "    return chosen_alpha, max_validation_score, test_score_at_chosen_alpha\n",
        "\n",
        "#%% Regular Linear Regression (For Baseline)\n",
        "\n",
        "lin = LinearRegression()\n",
        "\n",
        "lin_fit = lin.fit(X_train, Y_train)\n",
        "\n",
        "# finding mean squared error of model\n",
        "lin_preds = lin_fit.predict(X_test)\n",
        "lin_mse = mean_squared_error(Y_test, lin_preds).round(3)\n",
        "print(f\"The mean squared error for the linear regression model is {lin_mse}\")\n",
        "\n",
        "c=lin_fit.coef_\n",
        "\n",
        "lin_errors = Y_test - lin_preds\n",
        "\n",
        "plt.hist(lin_errors)\n",
        "\n",
        "#%% CV LASSO Grid Search - First Pass\n",
        "from sklearn.metrics import get_scorer\n",
        "\n",
        "# defining a cross validation object\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "# defining array of alphas from 0-1 with step of 0.1\n",
        "alphas = np.linspace(0,1,11) \n",
        "\n",
        "# running the functions\n",
        "chosen_alpha, max_validation_score, test_score_at_chosen_alpha = \\\n",
        "    regmodel_param_test(\n",
        "        alphas, X_train, Y_train, \n",
        "        cv, scoring = 'r2', model_name = 'LASSO', \n",
        "        X_test = X_test, y_test = Y_test, \n",
        "        draw_plot = True, filename = 'lasso_wide_search')\n",
        "    \n",
        "print(\"Chosen alpha: %.5f\" % \\\n",
        "    chosen_alpha)\n",
        "print(\"Validation score: %.5f\" % \\\n",
        "    max_validation_score)\n",
        "print(\"Test score at chosen alpha: %.5f\" % \\\n",
        "    test_score_at_chosen_alpha)\n",
        "    \n",
        "#%% CV LASSO Grid Search - Second Pass\n",
        "# defining a cross validation object\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "# defining array of alphas from 0-1 with step of 0.1\n",
        "alphas = np.linspace(0,0.2,21) \n",
        "\n",
        "# running the functions\n",
        "chosen_alpha, max_validation_score, test_score_at_chosen_alpha = \\\n",
        "    regmodel_param_test(\n",
        "        alphas, X_train, Y_train, \n",
        "        cv, scoring = 'r2', model_name = 'LASSO', \n",
        "        X_test = X_test, y_test = Y_test, \n",
        "        draw_plot = True, filename = 'lasso_wide_search')\n",
        "    \n",
        "print(\"Chosen alpha: %.5f\" % \\\n",
        "    chosen_alpha)\n",
        "print(\"Validation score: %.5f\" % \\\n",
        "    max_validation_score)\n",
        "print(\"Test score at chosen alpha: %.5f\" % \\\n",
        "    test_score_at_chosen_alpha)\n",
        "    \n",
        "#%% CV LASSO Grid Search - Third Pass\n",
        "# defining a cross validation object\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "# defining array of alphas from 0-1 with step of 0.1\n",
        "alphas = np.linspace(0.003,0.1,21) \n",
        "\n",
        "# running the functions\n",
        "chosen_alpha, max_validation_score, test_score_at_chosen_alpha = \\\n",
        "    regmodel_param_test(\n",
        "        alphas, X_train, Y_train, \n",
        "        cv, scoring = 'r2', model_name = 'LASSO', \n",
        "        X_test = X_test, y_test = Y_test, \n",
        "        draw_plot = True, filename = 'lasso_wide_search')\n",
        "    \n",
        "print(\"Chosen alpha: %.5f\" % \\\n",
        "    chosen_alpha)\n",
        "print(\"Validation score: %.5f\" % \\\n",
        "    max_validation_score)\n",
        "print(\"Test score at chosen alpha: %.5f\" % \\\n",
        "    test_score_at_chosen_alpha)\n",
        "\n",
        "#%% Getting Coefficients from Final Lasso Model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model = Lasso(alpha=chosen_alpha)\n",
        "\n",
        "fit = model.fit(X_train, Y_train)\n",
        "\n",
        "cdf = pd.DataFrame(fit.coef_, microwaves.columns[1:], columns=['Coefficients'])\n",
        "print(cdf)\n",
        "\n",
        "cdf[\"AbsCoef\"] = abs(cdf[\"Coefficients\"])\n",
        "\n",
        "# finding mean squared error of model\n",
        "preds = fit.predict(X_test)\n",
        "lasso_mse = mean_squared_error(Y_test, preds).round(3)\n",
        "print(f\"The mean squared error for the lasso model is {lasso_mse}\")\n",
        "\n",
        "selected_vars = cdf[cdf[\"Coefficients\"]>0]\n",
        "\n",
        "lasso_vars = list(selected_vars.index.values)\n",
        "lasso_vars.append(\"logprice\")\n",
        "\n",
        "microwaves_lassoed = microwaves[lasso_vars]\n",
        "\n",
        "Y = microwaves_lassoed[\"logprice\"]\n",
        "X = microwaves_lassoed.drop(columns=\"logprice\")\n",
        "\n",
        "# remove space from column names\n",
        "microwaves_lassoed.columns = microwaves_lassoed.columns.str.replace('_', '')\n",
        "microwaves_lassoed.columns = microwaves_lassoed.columns.str.replace(' ', '_')\n",
        "microwaves_lassoed.columns = microwaves_lassoed.columns.str.replace(',', '')\n",
        "microwaves_lassoed.columns = microwaves_lassoed.columns.str.replace('.', '')\n",
        "microwaves_lassoed.columns = microwaves_lassoed.columns.str.replace('-', '_')\n",
        "\n",
        "microwaves_lassoed = microwaves_lassoed.rename(columns={'1700_watts':'watts_1700','30_cu':'cu_30'})\n",
        "\n",
        "count = 1\n",
        "xs = \"\"\n",
        "for i in microwaves_lassoed.columns[:41]:\n",
        "    if count == len(microwaves_lassoed.columns[:41]):\n",
        "        xs = xs + i\n",
        "    else:\n",
        "        xs = xs + i + \" + \"\n",
        "        count += 1\n",
        "        \n",
        "formula = 'logprice ~ ' + xs\n",
        "        \n",
        "mod1 = smf.ols(formula = formula, data=microwaves_lassoed).fit()\n",
        "print(mod1.summary())\n",
        "\n",
        "#%% RandomForest model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "tree = RandomForestRegressor()\n",
        "\n",
        "tree_fit = tree.fit(X_train, Y_train)\n",
        "\n",
        "# finding mean squared error of model\n",
        "tree_preds = tree_fit.predict(X_test)\n",
        "rf_mse = mean_squared_error(Y_test, tree_preds).round(3)\n",
        "print(f\"The mean squared error for the random forest model is {rf_mse}\")\n",
        "\n",
        "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
        "\n",
        "rf_importance = pd.DataFrame(tree_fit.feature_importances_, microwaves.columns[1:], columns=['Importance']).sort_values(by=\"Importance\",ascending=False)\n",
        "len(rf_importance[rf_importance['Importance'] > 0])\n",
        "\n",
        "features = list(rf_importance.index)[0:20]\n",
        "importances = rf_importance.values.reshape(len(rf_importance))[0:20]\n",
        " \n",
        "# creating the bar plot\n",
        "plt.barh(features, importances, color ='green',\n",
        "        height = 0.4)\n",
        " \n",
        "plt.ylabel(\"Features\")\n",
        "plt.xlabel(\"Mean decrease in impurity\")\n",
        "plt.title(\"Top twenty feature importances using MDI\")\n",
        "plt.gca().invert_yaxis() \n",
        "#plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "#%% Boosted Regression Tree\n",
        "import xgboost as xgb\n",
        "\n",
        "# eta is equivalent to learning rate\n",
        "# max_depth is the max tree depth\n",
        "# subsample is proportion of sample used randomly for each tree\n",
        "# colsample_bytree is the proportion of columns randomly selected to be in each tree\n",
        "# alpha is regularization penalty (like lasso)\n",
        "boost = xgb.XGBRegressor(n_estimators=1000, max_depth = 5, eta = 0.1, subsample = 1, colsample_bytree = 0.91, alpha = 0)\n",
        "\n",
        "boost_fit = boost.fit(X_train, Y_train)\n",
        "\n",
        "print(boost.importance_type)\n",
        "\n",
        "# finding mean squared error of model\n",
        "boost_preds = boost_fit.predict(X_test)\n",
        "boost_mse = mean_squared_error(Y_test, boost_preds).round(3)\n",
        "print(f\"The mean squared error for the boosted regression model is {boost_mse}\")\n",
        "\n",
        "boost_importance = pd.DataFrame(boost_fit.feature_importances_, microwaves.columns[1:], columns=['Importance']).sort_values(by=\"Importance\",ascending=False)\n",
        "len(boost_importance[boost_importance['Importance'] > 0])\n",
        "len(boost_importance)\n",
        "\n",
        "boost_features = list(boost_importance.index)[0:20]\n",
        "boost_importances = boost_importance.values.reshape(len(boost_importance))[0:20]\n",
        "\n",
        "# creating the bar plot\n",
        "plt.barh(boost_features, boost_importances, color ='green',\n",
        "        height = 0.4)\n",
        " \n",
        "plt.ylabel(\"Features\")\n",
        "plt.xlabel(\"Gain\")\n",
        "plt.title(\"Top twenty feature importances using Gain\")\n",
        "plt.gca().invert_yaxis() \n",
        "#plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "mse = pd.DataFrame([lin_mse, lasso_mse, rf_mse, boost_mse],[\"Linear Regression - BoW\",\"LASSO - BoW\",\"RandomForest - BoW\",\"Gradient Boosted Tree - BoW\"])\n"
      ],
      "metadata": {
        "id": "CKB-S--wd2Xc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
